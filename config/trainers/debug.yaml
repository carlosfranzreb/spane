trainer: # config for the PTL trainer
  batch_size: 2
  max_epochs: 3
  num_workers: 10
  accumulate_grad_batches: 1
  limit_train_batches: 10 # how much of training dataset to use (pctg as float or n_batches as int)
  limit_val_batches: 10 # how much of validation dataset to use (pctg as float or n_batches as int)

optim:
  _target_: torch.optim.AdamW
  lr: 0.0002
  betas: [0.8, 0.99]

  sched:
    name: CosineAnnealing
    min_lr: 1e-5
    warmup_ratio: 0.02
