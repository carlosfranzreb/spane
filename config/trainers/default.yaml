trainer: # config for the PTL trainer
  batch_size: 2
  max_epochs: 20
  num_workers: 0
  accumulate_grad_batches: 1
  limit_train_batches: 1.0 # how much of training dataset to use (pctg as float or n_batches as int)
  limit_val_batches: 1.0 # how much of validation dataset to use (pctg as float or n_batches as int)

optim:
  _target_: torch.optim.AdamW
  lr: 0.0002
  betas: [0.8, 0.99]

  sched:
    name: CosineAnnealing
    min_lr: 1e-5
    warmup_ratio: 0.02

checkpointing: # define how and when to save checkpoints
  monitor: "val_loss" # name of the logged metric to measure the best models (TODO: make this a config param)
  mode: min # whether the monitored metric should be minimized or maximized (min or max as values)
