name: StarGANv2-VC
trainer:
  batch_size: 1
  max_epochs: 20
  num_workers: 10
  accumulate_grad_batches: 1
  limit_train_batches: 1.0
  limit_val_batches: 1.0
optim:
  _target_: torch.optim.AdamW
  lr: 0.0002
  betas:
  - 0.8
  - 0.99
  sched:
    name: CosineAnnealing
    min_lr: '1e-5'
    warmup_ratio: 0.02
checkpointing:
  monitor: val_loss
  mode: min
data:
  config:
    trainer: ${trainer}
    root_folder: /ds/audio
    sample_rate: ${sample_rate}
    trim_silence: false
    shuffle: false
    min_duration: 2
    max_duration: 30
  datasets:
    eval:
    - data/common_voice/cv-test_3utts.txt
    - data/librispeech/ls-test-clean.txt
    - data/edacc/edacc-test.txt
    - data/ravdess/ravdess.txt
    train_eval:
    - data/librispeech/ls-train-clean-100.txt
    - data/edacc/edacc-dev.txt
seed: 200
log_dir: logs/stargan
inference:
  run: true
  input:
    spectrogram: spectrogram
    target: target
resources:
  device: cuda
  n_devices: 1
  n_nodes: 1
  debug: false
sample_rate: 24000
featex:
  spectrogram:
    cls: src.featex.spectrogram.SpecExtractor
    n_mels: 80
    n_fft: 2048
    win_length: 1200
    hop_length: 300
featproc:
  star_gan:
    cls: src.featproc.star_gan.StarGAN
    init: checkpoints/stargan/Models/epoch_00150.pth
    config: checkpoints/stargan/Models/config.yml
    f0_ckpt: submodules/StarGANv2VC/Utils/JDC/bst.t7
    input:
      spectrogram: spectrogram
      source: source
      target: target
    n_targets: 20
  output:
    featproc:
    - spectrogram
    - target
    featex: []
synthesis:
  cls: src.synthesis.parallel_wavegan.ParallelWaveGAN
  init: checkpoints/stargan/Vocoder/checkpoint-400000steps.pkl
  sample_rate: 24000
  input:
    spectrogram: spectrogram
target_selection:
  cls: src.target_selection.random.RandomSelector
  consistent_targets: true
eval:
  config:
    seed: 3000
    baseline: false
    exp_folder: null
    sample_rate: ${synthesis.sample_rate}
    asv_reduce_dims: null
    spkid:
      cls: src.featex.spkid.spkid.SpkId
      toolkit: speechbrain
      path: speechbrain/spkrec-ecapa-voxceleb
      train: false
      trainer:
        max_epochs: 2
        batch_size: 4
        accumulate_grad_batches: 8
        log_every_n_steps: 1
        num_workers: 10
        limit_train_batches: 1.0
        limit_val_batches: 0
      optim:
        name: adamw
        lr: 1.0e-07
        weight_decay: 0.0002
        sched:
          name: CosineAnnealing
          warmup_ratio: 0.1
          min_lr: 0.0
  components:
    asv_ignorant:
      cls: src.evaluation.asv.spkid_plda.ASV
      scenario: ignorant
      train: true
      reduced_dims: ${eval.config.asv_reduce_dims}
      lda_ckpt: null
      plda_ckpt: null
      filter:
        min_dur: 4
        min_samples: 2
      spkid: ${eval.config.spkid}
    asv_lazy_informed:
      cls: src.evaluation.asv.spkid_plda.ASV
      scenario: lazy-informed
      train: true
      inference: ${inference}
      sample_rate: ${synthesis.sample_rate}
      reduced_dims: ${eval.config.asv_reduce_dims}
      lda_ckpt: null
      plda_ckpt: null
      filter:
        min_dur: 4
        min_samples: 2
      spkid: ${eval.config.spkid}
