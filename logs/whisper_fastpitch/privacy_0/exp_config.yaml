name: Multi-speaker FastPitch + corresponding HiFi-GAN
trainer:
  batch_size: 1
  max_epochs: 20
  num_workers: 10
  accumulate_grad_batches: 1
  limit_train_batches: 1.0
  limit_val_batches: 1.0
optim:
  _target_: torch.optim.AdamW
  lr: 0.0002
  betas:
  - 0.8
  - 0.99
  sched:
    name: CosineAnnealing
    min_lr: '1e-5'
    warmup_ratio: 0.02
checkpointing:
  monitor: val_loss
  mode: min
data:
  config:
    trainer: ${trainer}
    root_folder: /ds/audio
    sample_rate: ${sample_rate}
    trim_silence: false
    shuffle: false
    min_duration: 2
    max_duration: 30
  datasets:
    eval:
    - data/common_voice/cv-test_3utts.txt
    - data/librispeech/ls-test-clean.txt
    - data/edacc/edacc-test.txt
    - data/ravdess/ravdess.txt
    train_eval:
    - data/librispeech/ls-train-clean-100.txt
    - data/edacc/edacc-dev.txt
train: false
seed: 2000
log_dir: logs/whisper_fastpitch
inference:
  run: true
  input:
    spectrogram: spectrogram
    target: target
resources:
  device: cuda
  n_devices: 1
  n_nodes: 1
  debug: false
target_selection:
  cls: src.target_selection.random.RandomSelector
  consistent_targets: true
sample_rate: 16000
featex:
  whisper:
    cls: src.featex.asr.whisper.Whisper
    train: false
    size: small
    output: text
    batch_size: 4
featproc:
  fast_pitch:
    cls: src.featproc.fast_pitch.FastPitch
    init: tts_en_fastpitch_multispeaker
    input:
      text: whisper
      source: source
      target: target
    n_targets: 20
    target_selection: ${target_selection}
  output:
    featproc:
    - spectrogram
    - target
    featex: []
synthesis:
  cls: src.synthesis.hifigan.HifiGan
  init: tts_en_hifitts_hifigan_ft_fastpitch
  sample_rate: 44100
  input:
    spectrogram: spectrogram
eval:
  config:
    seed: 300
    baseline: false
    exp_folder: null
    sample_rate: ${synthesis.sample_rate}
    asv_reduce_dims: 200
    spkid:
      cls: src.featex.spkid.spkid.SpkId
      toolkit: speechbrain
      path: speechbrain/spkrec-xvect-voxceleb
      train: false
      trainer:
        max_epochs: 2
        batch_size: 4
        accumulate_grad_batches: 8
        log_every_n_steps: 1
        num_workers: 10
        limit_train_batches: 1.0
        limit_val_batches: 0
      optim:
        name: adamw
        lr: 1.0e-07
        weight_decay: 0.0002
        sched:
          name: CosineAnnealing
          warmup_ratio: 0.1
          min_lr: 0.0
  components:
    asv_ignorant:
      cls: src.evaluation.asv.spkid_plda.ASV
      scenario: ignorant
      train: true
      reduced_dims: ${eval.config.asv_reduce_dims}
      lda_ckpt: null
      plda_ckpt: null
      filter:
        min_dur: 4
        min_samples: 2
      spkid: ${eval.config.spkid}
    asv_lazy_informed:
      cls: src.evaluation.asv.spkid_plda.ASV
      scenario: lazy-informed
      train: true
      inference: ${inference}
      sample_rate: ${synthesis.sample_rate}
      reduced_dims: ${eval.config.asv_reduce_dims}
      lda_ckpt: null
      plda_ckpt: null
      filter:
        min_dur: 4
        min_samples: 2
      spkid: ${eval.config.spkid}
commit_hash: b11ae83
